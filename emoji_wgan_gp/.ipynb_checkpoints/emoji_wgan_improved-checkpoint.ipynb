{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH, CHANNEL = 64, 64, 3\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgpath = 'RGB_emoji/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, n, leak=0.2): \n",
    "    return tf.maximum(x, leak * x, name=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    images = glob.glob(imgpath+'*.jpg')\n",
    "    # print images    \n",
    "    all_images = tf.convert_to_tensor(images, dtype = tf.string)\n",
    "    \n",
    "    images_queue = tf.train.slice_input_producer([all_images],shuffle=False)\n",
    "                                        \n",
    "    content = tf.read_file(images_queue[0])\n",
    "    image = tf.image.decode_jpeg(content, channels = CHANNEL)\n",
    "    \n",
    "    size = [HEIGHT, WIDTH]\n",
    "    image = tf.image.resize_images(image, size)\n",
    "    image.set_shape([HEIGHT,WIDTH,CHANNEL])\n",
    "    \n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = ((image / 255.0)-0.5)*2\n",
    "    \n",
    "    images_batch = tf.train.shuffle_batch(\n",
    "                                    [image], batch_size = BATCH_SIZE,\n",
    "                                    num_threads = 4, capacity = 200 + 3* BATCH_SIZE,\n",
    "                                    min_after_dequeue = 200)\n",
    "    num_images = len(images)\n",
    "\n",
    "    return images_batch, num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generator(input, random_dim, is_train, reuse=False):\n",
    "    c4, c8, c16, c32, c64 = 512, 256, 128, 64, 32 # channel num\n",
    "    s4 = 4\n",
    "    output_dim = CHANNEL  # RGB image\n",
    "    with tf.variable_scope('gen') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        w1 = tf.get_variable('w1', shape=[random_dim, s4 * s4 * c4], dtype=tf.float32,\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b1 = tf.get_variable('b1', shape=[c4 * s4 * s4], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "        flat_conv1 = tf.add(tf.matmul(input, w1), b1, name='flat_conv1')\n",
    "\n",
    "        conv1 = tf.reshape(flat_conv1, shape=[-1, s4, s4, c4], name='conv1') # 4*4*512\n",
    "        bn1 = tf.contrib.layers.batch_norm(conv1, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn1')\n",
    "        act1 = tf.nn.relu(bn1, name='act1')\n",
    "        \n",
    "        # 8*8*256\n",
    "        #Convolution, bias, activation, repeat! \n",
    "        conv2 = tf.layers.conv2d_transpose(act1, c8, kernel_size=[3, 3], strides=[2, 2], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv2')\n",
    "        bn2 = tf.contrib.layers.batch_norm(conv2, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn2')\n",
    "        act2 = tf.nn.relu(bn2, name='act2')\n",
    "        # 16*16*128\n",
    "        conv3 = tf.layers.conv2d_transpose(act2, c16, kernel_size=[3, 3], strides=[2, 2], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv3')\n",
    "        bn3 = tf.contrib.layers.batch_norm(conv3, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn3')\n",
    "        act3 = tf.nn.relu(bn3, name='act3')\n",
    "        # 32*32*64\n",
    "        conv4 = tf.layers.conv2d_transpose(act3, c32, kernel_size=[3, 3], strides=[2, 2], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv4')\n",
    "        bn4 = tf.contrib.layers.batch_norm(conv4, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn4')\n",
    "        act4 = tf.nn.relu(bn4, name='act4')\n",
    "        # 64*64*32\n",
    "        conv5 = tf.layers.conv2d_transpose(act4, c64, kernel_size=[3, 3], strides=[2, 2], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv5')\n",
    "        bn5 = tf.contrib.layers.batch_norm(conv5, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn5')\n",
    "        act5 = tf.nn.relu(bn5, name='act5')\n",
    "        \n",
    "        #64*64*16\n",
    "        conv6 = tf.layers.conv2d_transpose(act5, 16, kernel_size=[3, 3], strides=[1, 1], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv6')\n",
    "        bn6 = tf.contrib.layers.batch_norm(conv6, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn6')\n",
    "        act6 = tf.nn.relu(bn6, name='act6')\n",
    "        \n",
    "        #64*64*8\n",
    "        conv7 = tf.layers.conv2d_transpose(act6, 8, kernel_size=[3, 3], strides=[1, 1], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv7')\n",
    "        bn7 = tf.contrib.layers.batch_norm(conv7, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn7')\n",
    "        act7 = tf.nn.relu(bn7, name='act7')\n",
    "        \n",
    "        #64*64*8\n",
    "        conv8 = tf.layers.conv2d_transpose(act7, 8, kernel_size=[3, 3], strides=[1, 1], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv8')\n",
    "        bn8 = tf.contrib.layers.batch_norm(conv8, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn8')\n",
    "        act8 = tf.nn.relu(bn8, name='act8')\n",
    "        \n",
    "        #64*64*3\n",
    "        conv9 = tf.layers.conv2d_transpose(act8, 3, kernel_size=[3, 3], strides=[1, 1], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv9')\n",
    "        \n",
    "        \n",
    "        \n",
    "        act9 = tf.nn.tanh(conv9, name='act9')\n",
    "        return act9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def discriminator(input, is_train, reuse=False):\n",
    "    c1,c2, c4, c8, c16 = 32, 64, 128, 256, 512  # channel num: 32,64, 128, 256, 512\n",
    "    with tf.variable_scope('dis') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "        #Convolution, activation, bias, repeat! \n",
    "        conv0 = tf.layers.conv2d(input, c1, kernel_size=[3, 3], strides=[2, 2], padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name='conv0')\n",
    "        ln0 = tf.contrib.layers.layer_norm(conv0, center=True, scale=True, scope = 'ln0')\n",
    "        act0 = lrelu(ln0, n='act0')\n",
    "        \n",
    "        #Convolution, activation, bias, repeat! \n",
    "        conv1 = tf.layers.conv2d(act0, c2, kernel_size=[3, 3], strides=[2, 2], padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name='conv1')\n",
    "        ln1 = tf.contrib.layers.layer_norm(conv1, center=True, scale=True, scope = 'ln1')\n",
    "        act1 = lrelu(ln1, n='act1')\n",
    "         #Convolution, activation, bias, repeat! \n",
    "        conv2 = tf.layers.conv2d(act1, c4, kernel_size=[3, 3], strides=[2, 2], padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name='conv2')\n",
    "        ln2 = tf.contrib.layers.layer_norm(conv2, center=True, scale=True, scope='ln2')\n",
    "        act2 = lrelu(ln2, n='act2')\n",
    "        #Convolution, activation, bias, repeat! \n",
    "        conv3 = tf.layers.conv2d(act2, c8, kernel_size=[3, 3], strides=[2, 2], padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name='conv3')\n",
    "        ln3 = tf.contrib.layers.layer_norm(conv3, center=True, scale=True, scope='ln3')\n",
    "        act3 = lrelu(ln3, n='act3')\n",
    "         #Convolution, activation, bias, repeat! \n",
    "        conv4 = tf.layers.conv2d(act3, c16, kernel_size=[3, 3], strides=[2, 2], padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name='conv4')\n",
    "        ln4 = tf.contrib.layers.layer_norm(conv4,  center=True, scale=True, scope='ln4')\n",
    "        act4 = lrelu(ln4, n='act4')\n",
    "               \n",
    "        # start from act4\n",
    "        dim = int(np.prod(act4.get_shape()[1:]))\n",
    "        fc1 = tf.reshape(act4, shape=[-1, dim], name='fc1')\n",
    "        \n",
    "        w2 = tf.get_variable('w2', shape=[fc1.shape[-1], 1], dtype=tf.float32,\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b2 = tf.get_variable('b2', shape=[1], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # wgan just get rid of the sigmoid\n",
    "        logits = tf.add(tf.matmul(fc1, w2), b2, name='logits')\n",
    "\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('input'):\n",
    "    #real and fake image placholders\n",
    "    real_image = tf.placeholder(tf.float32, shape = [None, HEIGHT, WIDTH, CHANNEL], name='real_image')\n",
    "    random_input = tf.placeholder(tf.float32, shape=[None, random_dim], name='rand_input')\n",
    "    is_train = tf.placeholder(tf.bool, name='is_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wgan\n",
    "fake_image = generator(random_input, random_dim, is_train)\n",
    "    \n",
    "real_result = discriminator(real_image, is_train)\n",
    "fake_result = discriminator(fake_image, is_train, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_loss = tf.reduce_mean(fake_result) - tf.reduce_mean(real_result)  # This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(fake_result)  # This optimizes the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gradient penalty\n",
    "LAMBDA = 500\n",
    "\n",
    "alpha = tf.random_uniform(\n",
    "        shape=[], \n",
    "        minval=0.,\n",
    "        maxval=1.\n",
    ")\n",
    "differences = fake_image - real_image\n",
    "interpolates = real_image + tf.multiply(alpha,differences)\n",
    "gradients = tf.gradients(discriminator(interpolates,is_train, reuse=True), [interpolates])[0]\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))\n",
    "\n",
    "gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "d_loss += LAMBDA*gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'dis' in var.name]\n",
    "g_vars = [var for var in t_vars if 'gen' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_d = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(d_loss, var_list=d_vars)\n",
    "trainer_g = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "image_batch, samples_num = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_num = int(samples_num / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_path = 'model/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_path = 'logs/'\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_path = 'example/'\n",
    "if not os.path.exists(example_path):\n",
    "    os.makedirs(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 90000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_iters = 5\n",
    "g_iters = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    writer_d_loss = tf.summary.FileWriter(log_path+'/d_loss',sess.graph)\n",
    "    writer_g_loss = tf.summary.FileWriter(log_path+'/g_loss')\n",
    "\n",
    "    loss_var = tf.Variable(0.0)\n",
    "    tf.summary.scalar(\"loss\", loss_var)\n",
    "    write_op = tf.summary.merge_all()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    start = 0\n",
    "    ckpt = tf.train.latest_checkpoint(save_path)\n",
    "    if ckpt:\n",
    "        print 'restore:'+str(ckpt)\n",
    "        saver.restore(sess, ckpt)\n",
    "        start = int(ckpt.split('-')[-1])+1\n",
    "        \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for step in range(start,EPOCH*batch_num):\n",
    "        train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
    "        \n",
    "        train_image = sess.run(image_batch)\n",
    "        \n",
    "        for k in range(d_iters):\n",
    "            _, dLoss = sess.run([trainer_d, d_loss],\n",
    "                                    feed_dict={random_input: train_noise, real_image: train_image, is_train: True})\n",
    "        if d_iters!=0:\n",
    "            print 'dLoss:'+str(dLoss)\n",
    "\n",
    "\n",
    "        for k in range(g_iters):\n",
    "            _, gLoss = sess.run([trainer_g, g_loss],\n",
    "                                    feed_dict={random_input: train_noise, is_train: True})\n",
    "        if g_iters!=0:\n",
    "            print 'gLoss:'+str(gLoss)\n",
    "        \n",
    "            \n",
    "                \n",
    "        if step%25 == 0:\n",
    "            saver.save(sess, save_path + 'model.ckpt', global_step=step)\n",
    "            \n",
    "            if d_iters!=0:\n",
    "                summary = sess.run(write_op, {loss_var: dLoss})\n",
    "                writer_d_loss.add_summary(summary, step)\n",
    "                writer_d_loss.flush()\n",
    "            if g_iters!=0:\n",
    "                summary = sess.run(write_op, {loss_var: gLoss})\n",
    "                writer_g_loss.add_summary(summary, step)\n",
    "                writer_g_loss.flush()\n",
    "            \n",
    "        if step%25 == 0:\n",
    "            # sample_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
    "            imgtest = sess.run(fake_image, feed_dict={random_input: train_noise, is_train: False})\n",
    "            save_images(imgtest, [8,8] , example_path+'/step_' + str(step) + '.jpg')\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    random_dim = 128\n",
    "    gen_path = 'testgen/'\n",
    "    if not os.path.exists(gen_path):\n",
    "        os.makedirs(gen_path)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.variable_scope('input'):\n",
    "        #real and fake image placholders\n",
    "        real_image = tf.placeholder(tf.float32, shape = [None, HEIGHT, WIDTH, CHANNEL], name='real_image')\n",
    "        random_input = tf.placeholder(tf.float32, shape=[None, random_dim], name='rand_input')\n",
    "        is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "    # wgan\n",
    "    fake_image = generator(random_input, random_dim, is_train)\n",
    "\n",
    "    real_result = discriminator(real_image, is_train)\n",
    "    fake_result = discriminator(fake_image, is_train, reuse=True)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        ckpt = tf.train.latest_checkpoint('model/')\n",
    "        saver.restore(sess, ckpt)\n",
    "        \n",
    "        sample_noise1 = np.random.uniform(-1.0, 1.0, size=[1, random_dim]).astype(np.float32)\n",
    "        imgtest = sess.run(fake_image, feed_dict={random_input: sample_noise1, is_train: False})\n",
    "        save_images(imgtest, [1,1] , gen_path+'/example1.jpg')\n",
    "        \n",
    "        sample_noise2 = np.random.uniform(-1.0, 1.0, size=[1, random_dim]).astype(np.float32)\n",
    "        imgtest = sess.run(fake_image, feed_dict={random_input: sample_noise2, is_train: False})\n",
    "        save_images(imgtest, [1,1] , gen_path+'/example2.jpg')\n",
    "        \n",
    "        sample_noise3 = np.random.uniform(-1.0, 1.0, size=[1, random_dim]).astype(np.float32)\n",
    "        imgtest = sess.run(fake_image, feed_dict={random_input: sample_noise3, is_train: False})\n",
    "        save_images(imgtest, [1,1] , gen_path+'/example3.jpg')\n",
    "        \n",
    "        sample_noise4 = sample_noise1 - sample_noise2 + sample_noise3\n",
    "        imgtest = sess.run(fake_image, feed_dict={random_input: sample_noise4, is_train: False})\n",
    "        save_images(imgtest, [1,1] , gen_path+'/example4.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
